From 7c1d4e0f094b50f4f0ca86932c10164869ed888f Mon Sep 17 00:00:00 2001
From: Andrew Eikum <aeikum@codeweavers.com>
Date: Tue, 28 Aug 2018 12:57:32 -0500
Subject: [PATCH] ntdll,server: Never use CLOCK_MONOTONIC_RAW

Using CLOCK_MONOTONIC avoids a kernel call.
---
 dlls/ntdll/time.c | 2 +-
 server/request.c  | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/dlls/ntdll/time.c b/dlls/ntdll/time.c
index a4762facc5b..3a282cb5b76 100644
--- a/dlls/ntdll/time.c
+++ b/dlls/ntdll/time.c
@@ -114,7 +114,7 @@ static ULONGLONG monotonic_counter(void)
     return mach_absolute_time() * timebase.numer / timebase.denom / 100;
 #elif defined(HAVE_CLOCK_GETTIME)
     struct timespec ts;
-#ifdef CLOCK_MONOTONIC_RAW
+#if 0
     if (!clock_gettime( CLOCK_MONOTONIC_RAW, &ts ))
         return ts.tv_sec * (ULONGLONG)TICKSPERSEC + ts.tv_nsec / 100;
 #endif
diff --git a/server/request.c b/server/request.c
index 7420e4a5a4e..508b087379b 100644
--- a/server/request.c
+++ b/server/request.c
@@ -532,7 +532,7 @@ unsigned int get_tick_count(void)
     return mach_absolute_time() * timebase.numer / timebase.denom / 1000000;
 #elif defined(HAVE_CLOCK_GETTIME)
     struct timespec ts;
-#ifdef CLOCK_MONOTONIC_RAW
+#if 0
     if (!clock_gettime( CLOCK_MONOTONIC_RAW, &ts ))
         return ts.tv_sec * 1000 + ts.tv_nsec / 1000000;
 #endif
From 6066c661b3dae8d0d15d339a902a6a1d939c5303 Mon Sep 17 00:00:00 2001
From: Sebastian Lackner <sebastian@fds-team.de>
Date: Sat, 22 Jul 2017 06:41:53 +0200
Subject: [PATCH] ntdll: Add helper function to delete free blocks.

---
 dlls/ntdll/heap.c | 23 +++++++++++++++++------
 1 file changed, 17 insertions(+), 6 deletions(-)

diff --git a/dlls/ntdll/heap.c b/dlls/ntdll/heap.c
index bdadc7932d9..75d093f4f06 100644
--- a/dlls/ntdll/heap.c
+++ b/dlls/ntdll/heap.c
@@ -494,6 +494,17 @@ static inline void HEAP_InsertFreeBlock( HEAP *heap, ARENA_FREE *pArena, BOOL la
 }
 
 
+/***********************************************************************
+ *           HEAP_DeleteFreeBlock
+ *
+ * Delete a free block from the free list.
+ */
+static inline void HEAP_DeleteFreeBlock( HEAP *heap, ARENA_FREE *pArena )
+{
+    list_remove( &pArena->entry );
+}
+
+
 /***********************************************************************
  *           HEAP_FindSubHeap
  * Find the sub-heap containing a given address.
@@ -602,7 +613,7 @@ static void HEAP_CreateFreeBlock( SUBHEAP *subheap, void *ptr, SIZE_T size )
     {
         /* Remove the next arena from the free list */
         ARENA_FREE *pNext = (ARENA_FREE *)((char *)ptr + size);
-        list_remove( &pNext->entry );
+        HEAP_DeleteFreeBlock( subheap->heap, pNext );
         size += (pNext->size & ARENA_SIZE_MASK) + sizeof(*pNext);
         mark_block_free( pNext, sizeof(ARENA_FREE), flags );
     }
@@ -657,7 +668,7 @@ static void HEAP_MakeInUseBlockFree( SUBHEAP *subheap, ARENA_INUSE *pArena )
         pFree = *((ARENA_FREE **)pArena - 1);
         size += (pFree->size & ARENA_SIZE_MASK) + sizeof(ARENA_FREE);
         /* Remove it from the free list */
-        list_remove( &pFree->entry );
+        HEAP_DeleteFreeBlock( heap, pFree );
     }
     else pFree = (ARENA_FREE *)pArena;
 
@@ -677,7 +688,7 @@ static void HEAP_MakeInUseBlockFree( SUBHEAP *subheap, ARENA_INUSE *pArena )
 
         size = 0;
         /* Remove the free block from the list */
-        list_remove( &pFree->entry );
+        HEAP_DeleteFreeBlock( heap, pFree );
         /* Remove the subheap from the list */
         list_remove( &subheap->entry );
         /* Free the memory */
@@ -1707,7 +1718,7 @@ void * WINAPI DECLSPEC_HOTPATCH RtlAllocateHeap( HANDLE heap, ULONG flags, SIZE_
 
     /* Remove the arena from the free list */
 
-    list_remove( &pArena->entry );
+    HEAP_DeleteFreeBlock( heapPtr, pArena );
 
     /* Build the in-use arena */
 
@@ -1864,7 +1875,7 @@ PVOID WINAPI RtlReAllocateHeap( HANDLE heap, ULONG flags, PVOID ptr, SIZE_T size
         {
             /* The next block is free and large enough */
             ARENA_FREE *pFree = (ARENA_FREE *)pNext;
-            list_remove( &pFree->entry );
+            HEAP_DeleteFreeBlock( heapPtr, pFree );
             pArena->size += (pFree->size & ARENA_SIZE_MASK) + sizeof(*pFree);
             if (!HEAP_Commit( subheap, pArena, rounded_size )) goto oom;
             notify_realloc( pArena + 1, oldActualSize, size );
@@ -1882,7 +1893,7 @@ PVOID WINAPI RtlReAllocateHeap( HANDLE heap, ULONG flags, PVOID ptr, SIZE_T size
 
             /* Build the in-use arena */
 
-            list_remove( &pNew->entry );
+            HEAP_DeleteFreeBlock( heapPtr, pNew );
             pInUse = (ARENA_INUSE *)pNew;
             pInUse->size = (pInUse->size & ~ARENA_FLAG_FREE)
                            + sizeof(ARENA_FREE) - sizeof(ARENA_INUSE);
From 63fd22f6227f0eecc33901622d98287c3f7bbc1e Mon Sep 17 00:00:00 2001
From: Sebastian Lackner <sebastian@fds-team.de>
Date: Sat, 22 Jul 2017 07:21:45 +0200
Subject: [PATCH] ntdll: Improve heap allocation performance. (v2)

---
 configure.ac      |   9 ++
 dlls/ntdll/heap.c | 321 ++++++++++++++++++++++++++++++++--------------
 2 files changed, 234 insertions(+), 96 deletions(-)

diff --git a/configure.ac b/configure.ac
index 4906cf548a8..dd6d7de0516 100644
--- a/configure.ac
+++ b/configure.ac
@@ -2798,6 +2798,15 @@ then
     AC_DEFINE(HAVE___BUILTIN_CLZ, 1, [Define to 1 if you have the `__builtin_clz' built-in function.])
 fi
 
+dnl Check for __builtin_ctzl
+AC_CACHE_CHECK([for __builtin_ctzl], ac_cv_have___builtin_ctzl,
+               AC_LINK_IFELSE([AC_LANG_PROGRAM(,[[return __builtin_ctzl(1)]])],
+               [ac_cv_have___builtin_ctzl="yes"], [ac_cv_have___builtin_ctzl="no"]))
+if test "$ac_cv_have___builtin_ctzl" = "yes"
+then
+    AC_DEFINE(HAVE___BUILTIN_CTZL, 1, [Define to 1 if you have the `__builtin_ctzl' built-in function.])
+fi
+
 dnl Check for __builtin_popcount
 AC_CACHE_CHECK([for __builtin_popcount], ac_cv_have___builtin_popcount,
                AC_LINK_IFELSE([AC_LANG_PROGRAM(,[[return __builtin_popcount(1)]])],
diff --git a/dlls/ntdll/heap.c b/dlls/ntdll/heap.c
index 75d093f4f06..a48b45ac656 100644
--- a/dlls/ntdll/heap.c
+++ b/dlls/ntdll/heap.c
@@ -3,6 +3,7 @@
  *
  * Copyright 1996 Alexandre Julliard
  * Copyright 1998 Ulrich Weigand
+ * Copyright 2017 Sebastian Lackner
  *
  * This library is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
@@ -41,6 +42,7 @@
 #include "winternl.h"
 #include "ntdll_misc.h"
 #include "wine/list.h"
+#include "wine/rbtree.h"
 #include "wine/debug.h"
 #include "wine/server.h"
 
@@ -62,7 +64,11 @@ typedef struct tagARENA_FREE
 {
     DWORD                 size;     /* Block size; must be the first field */
     DWORD                 magic;    /* Magic number */
-    struct list           entry;    /* Entry in free list */
+    union
+    {
+        struct list list;           /* Entry in free list */
+        struct wine_rb_entry tree;  /* Entry in free tree */
+    } entry;
 } ARENA_FREE;
 
 typedef struct
@@ -75,9 +81,11 @@ typedef struct
     DWORD                 magic;      /* these must remain at the end of the structure */
 } ARENA_LARGE;
 
-#define ARENA_FLAG_FREE        0x00000001  /* flags OR'ed with arena size */
-#define ARENA_FLAG_PREV_FREE   0x00000002
-#define ARENA_SIZE_MASK        (~3)
+#define ARENA_FLAG_FREE_LIST   0x00000001  /* flags OR'ed with arena size */
+#define ARENA_FLAG_FREE_TREE   0x00000002
+#define ARENA_FLAG_FREE        (ARENA_FLAG_FREE_LIST | ARENA_FLAG_FREE_TREE)
+#define ARENA_FLAG_PREV_FREE   0x00000004
+#define ARENA_SIZE_MASK        (~7)
 #define ARENA_LARGE_SIZE       0xfedcba90  /* magic value for 'size' field in large blocks */
 
 /* Value for arena 'magic' field */
@@ -95,6 +103,8 @@ typedef struct
 #define LARGE_ALIGNMENT        16  /* large blocks have stricter alignment */
 #define ARENA_OFFSET           (ALIGNMENT - sizeof(ARENA_INUSE))
 
+C_ASSERT( (sizeof(ARENA_INUSE) & ~ARENA_SIZE_MASK) == 0 );
+C_ASSERT( (sizeof(ARENA_FREE) & ~ARENA_SIZE_MASK) == 0 );
 C_ASSERT( sizeof(ARENA_LARGE) % LARGE_ALIGNMENT == 0 );
 
 #define ROUND_SIZE(size)       ((((size) + ALIGNMENT - 1) & ~(ALIGNMENT-1)) + ARENA_OFFSET)
@@ -103,9 +113,7 @@ C_ASSERT( sizeof(ARENA_LARGE) % LARGE_ALIGNMENT == 0 );
 #define NOISY                  0           /* Report all errors  */
 
 /* minimum data size (without arenas) of an allocated block */
-/* make sure that it's larger than a free list entry */
-#define HEAP_MIN_DATA_SIZE    ROUND_SIZE(2 * sizeof(struct list))
-#define HEAP_MIN_ARENA_SIZE   (HEAP_MIN_DATA_SIZE + sizeof(ARENA_INUSE))
+#define HEAP_MIN_DATA_SIZE    ROUND_SIZE(sizeof(ARENA_FREE) + sizeof(ARENA_FREE*) - sizeof(ARENA_INUSE))
 /* minimum size that must remain to shrink an allocated block */
 #define HEAP_MIN_SHRINK_SIZE  (HEAP_MIN_DATA_SIZE+sizeof(ARENA_FREE))
 /* minimum size to start allocating large blocks */
@@ -114,23 +122,14 @@ C_ASSERT( sizeof(ARENA_LARGE) % LARGE_ALIGNMENT == 0 );
 #define HEAP_TAIL_EXTRA_SIZE(flags) \
     ((flags & HEAP_TAIL_CHECKING_ENABLED) || RUNNING_ON_VALGRIND ? ALIGNMENT : 0)
 
-/* There will be a free list bucket for every arena size up to and including this value */
-#define HEAP_MAX_SMALL_FREE_LIST 0x100
-C_ASSERT( HEAP_MAX_SMALL_FREE_LIST % ALIGNMENT == 0 );
-#define HEAP_NB_SMALL_FREE_LISTS (((HEAP_MAX_SMALL_FREE_LIST - HEAP_MIN_ARENA_SIZE) / ALIGNMENT) + 1)
-
-/* Max size of the blocks on the free lists above HEAP_MAX_SMALL_FREE_LIST */
-static const SIZE_T HEAP_freeListSizes[] =
-{
-    0x200, 0x400, 0x1000, ~0UL
-};
-#define HEAP_NB_FREE_LISTS (ARRAY_SIZE( HEAP_freeListSizes ) + HEAP_NB_SMALL_FREE_LISTS)
-
-typedef union
-{
-    ARENA_FREE  arena;
-    void       *alignment[4];
-} FREE_LIST_ENTRY;
+/* size of the blocks on the free lists */
+#define HEAP_FREELIST_SIZE(index) \
+    ((DWORD)(((index) * ALIGNMENT) + HEAP_MIN_DATA_SIZE + sizeof(ARENA_INUSE)))
+/* returns index for a given block size */
+#define HEAP_SIZE_TO_FREELIST_INDEX(size) \
+    (((size) - HEAP_MIN_DATA_SIZE - sizeof(ARENA_INUSE)) / ALIGNMENT)
+/* number of free lists */
+#define HEAP_NB_FREE_LISTS  128
 
 struct tagHEAP;
 
@@ -163,9 +162,17 @@ typedef struct tagHEAP
     DWORD            pending_pos;   /* Position in pending free requests ring */
     ARENA_INUSE    **pending_free;  /* Ring buffer for pending free requests */
     RTL_CRITICAL_SECTION critSection; /* Critical section for serialization */
-    FREE_LIST_ENTRY *freeList;      /* Free lists */
+    struct list     *freeList;      /* Free lists */
+    struct wine_rb_tree freeTree;   /* Free tree */
+    unsigned long    freeMask[HEAP_NB_FREE_LISTS / (8 * sizeof(unsigned long))];
 } HEAP;
 
+#define HEAP_FREEMASK_BLOCK    (8 * sizeof(unsigned long))
+#define HEAP_FREEMASK_INDEX(x) ((x) / HEAP_FREEMASK_BLOCK)
+#define HEAP_FREEMASK_BIT(x)   (1UL << ((x) & (HEAP_FREEMASK_BLOCK - 1)))
+
+C_ASSERT( HEAP_NB_FREE_LISTS % HEAP_FREEMASK_BLOCK == 0 );
+
 #define HEAP_MAGIC       ((DWORD)('H' | ('E'<<8) | ('A'<<16) | ('P'<<24)))
 
 #define HEAP_DEF_SIZE        0x110000   /* Default heap size = 1Mb + 64Kb */
@@ -182,6 +189,30 @@ static HEAP *processHeap;  /* main process heap */
 
 static BOOL HEAP_IsRealArena( HEAP *heapPtr, DWORD flags, LPCVOID block, BOOL quiet );
 
+/* get arena size for an rb tree entry */
+static inline DWORD get_arena_size( const struct wine_rb_entry *entry )
+{
+    ARENA_FREE *arena = WINE_RB_ENTRY_VALUE( entry, ARENA_FREE, entry.tree );
+    return (arena->size & ARENA_SIZE_MASK);
+}
+
+/* return number of trailing 0-bits in x */
+static inline int ctzl(unsigned long x)
+{
+#ifdef HAVE___BUILTIN_CTZL
+    return __builtin_ctzl(x);
+#else
+    int c = 1;
+    if (!(x & 0xffffffff)) { x >>= 32; c += 32; }
+    if (!(x & 0x0000ffff)) { x >>= 16; c += 16; }
+    if (!(x & 0x000000ff)) { x >>=  8; c +=  8; }
+    if (!(x & 0x0000000f)) { x >>=  4; c +=  4; }
+    if (!(x & 0x00000003)) { x >>=  2; c +=  2; }
+    c -= (x & 0x00000001);
+    return c;
+#endif
+}
+
 /* mark a block of memory as free for debugging purposes */
 static inline void mark_block_free( void *ptr, SIZE_T size, DWORD flags )
 {
@@ -303,20 +334,6 @@ static void subheap_notify_free_all(SUBHEAP const *subheap)
 #endif
 }
 
-/* locate a free list entry of the appropriate size */
-/* size is the size of the whole block including the arena header */
-static inline unsigned int get_freelist_index( SIZE_T size )
-{
-    unsigned int i;
-
-    if (size <= HEAP_MAX_SMALL_FREE_LIST)
-        return (size - HEAP_MIN_ARENA_SIZE) / ALIGNMENT;
-
-    for (i = HEAP_NB_SMALL_FREE_LISTS; i < HEAP_NB_FREE_LISTS - 1; i++)
-        if (size <= HEAP_freeListSizes[i - HEAP_NB_SMALL_FREE_LISTS]) break;
-    return i;
-}
-
 /* get the memory protection type to use for a given heap */
 static inline ULONG get_protection_type( DWORD flags )
 {
@@ -344,13 +361,31 @@ static void HEAP_Dump( HEAP *heap )
     TRACE( "Next: %p  Sub-heaps:", LIST_ENTRY( heap->entry.next, HEAP, entry ) );
     LIST_FOR_EACH_ENTRY( subheap, &heap->subheap_list, SUBHEAP, entry ) TRACE( " %p", subheap );
 
-    TRACE( "\nFree lists:\n Block   Stat   Size    Id\n" );
+    TRACE( "\nFree lists:\n" );
     for (i = 0; i < HEAP_NB_FREE_LISTS; i++)
-        TRACE( "%p free %08lx prev=%p next=%p\n",
-                 &heap->freeList[i].arena, i < HEAP_NB_SMALL_FREE_LISTS ?
-                 HEAP_MIN_ARENA_SIZE + i * ALIGNMENT : HEAP_freeListSizes[i - HEAP_NB_SMALL_FREE_LISTS],
-                 LIST_ENTRY( heap->freeList[i].arena.entry.prev, ARENA_FREE, entry ),
-                 LIST_ENTRY( heap->freeList[i].arena.entry.next, ARENA_FREE, entry ));
+    {
+        BOOL empty = !(heap->freeMask[ HEAP_FREEMASK_INDEX( i ) ] & HEAP_FREEMASK_BIT( i ));
+
+        TRACE( "free %08x: ", HEAP_FREELIST_SIZE( i ) );
+        if (!empty && !list_empty( &heap->freeList[i] ))
+        {
+            TRACE( "head=%p tail=%p\n",
+                     LIST_ENTRY( heap->freeList[i].next, ARENA_FREE, entry.list ),
+                     LIST_ENTRY( heap->freeList[i].prev, ARENA_FREE, entry.list ));
+        }
+        else if (empty && list_empty( &heap->freeList[i] ))
+        {
+            TRACE( "(empty)\n" );
+        }
+        else
+        {
+            TRACE( "(corrupted)\n" );
+        }
+    }
+
+    TRACE( "free %08x: root=%p\n",
+             HEAP_FREELIST_SIZE( HEAP_NB_FREE_LISTS ),
+             heap->freeTree.root ? LIST_ENTRY( heap->freeTree.root, ARENA_FREE, entry.tree ) : NULL);
 
     LIST_FOR_EACH_ENTRY( subheap, &heap->subheap_list, SUBHEAP, entry )
     {
@@ -365,11 +400,32 @@ static void HEAP_Dump( HEAP *heap )
             if (*(DWORD *)ptr & ARENA_FLAG_FREE)
             {
                 ARENA_FREE *pArena = (ARENA_FREE *)ptr;
-                TRACE( "%p %08x free %08x prev=%p next=%p\n",
-                         pArena, pArena->magic,
-                         pArena->size & ARENA_SIZE_MASK,
-                         LIST_ENTRY( pArena->entry.prev, ARENA_FREE, entry ),
-                         LIST_ENTRY( pArena->entry.next, ARENA_FREE, entry ) );
+                if ((pArena->size & ARENA_FLAG_FREE) == ARENA_FLAG_FREE_LIST)
+                {
+                    TRACE( "%p %08x free %08x prev=%p next=%p\n",
+                              pArena, pArena->magic, pArena->size & ARENA_SIZE_MASK,
+                              LIST_ENTRY( pArena->entry.list.prev, ARENA_FREE, entry.list ),
+                              LIST_ENTRY( pArena->entry.list.next, ARENA_FREE, entry.list ) );
+                }
+                else if ((pArena->size & ARENA_FLAG_FREE) == ARENA_FLAG_FREE_TREE)
+                {
+                    ARENA_FREE *parent = NULL, *left = NULL, *right = NULL;
+
+                    if (pArena->entry.tree.parent)
+                        parent = WINE_RB_ENTRY_VALUE( pArena->entry.tree.parent, ARENA_FREE, entry.tree );
+                    if (pArena->entry.tree.left)
+                        left = WINE_RB_ENTRY_VALUE( pArena->entry.tree.left, ARENA_FREE, entry.tree );
+                    if (pArena->entry.tree.right)
+                        right = WINE_RB_ENTRY_VALUE( pArena->entry.tree.right, ARENA_FREE, entry.tree );
+
+                    TRACE( "%p %08x free %08x parent=%p left=%p right=%p\n",
+                             pArena, pArena->magic, pArena->size & ARENA_SIZE_MASK, parent, left, right );
+                }
+                else
+                {
+                    TRACE( "%p %08x free %08x corrupted\n",
+                             pArena, pArena->magic, pArena->size & ARENA_SIZE_MASK );
+                }
                 ptr += sizeof(*pArena) + (pArena->size & ARENA_SIZE_MASK);
                 arenaSize += sizeof(ARENA_FREE);
                 freeSize += pArena->size & ARENA_SIZE_MASK;
@@ -477,20 +533,19 @@ static HEAP *HEAP_GetPtr(
  */
 static inline void HEAP_InsertFreeBlock( HEAP *heap, ARENA_FREE *pArena, BOOL last )
 {
-    FREE_LIST_ENTRY *pEntry = heap->freeList + get_freelist_index( pArena->size + sizeof(*pArena) );
-    if (last)
+    SIZE_T index = HEAP_SIZE_TO_FREELIST_INDEX( pArena->size + sizeof(*pArena) );
+
+    if (index < HEAP_NB_FREE_LISTS)
     {
-        /* insert at end of free list, i.e. before the next free list entry */
-        pEntry++;
-        if (pEntry == &heap->freeList[HEAP_NB_FREE_LISTS]) pEntry = heap->freeList;
-        list_add_before( &pEntry->arena.entry, &pArena->entry );
+        list_add_tail( &heap->freeList[index], &pArena->entry.list );
+        heap->freeMask[ HEAP_FREEMASK_INDEX( index ) ] |= HEAP_FREEMASK_BIT( index );
+        pArena->size |= ARENA_FLAG_FREE_LIST;
     }
     else
     {
-        /* insert at head of free list */
-        list_add_after( &pEntry->arena.entry, &pArena->entry );
+        wine_rb_put( &heap->freeTree, &pArena->size, &pArena->entry.tree );
+        pArena->size |= ARENA_FLAG_FREE_TREE;
     }
-    pArena->size |= ARENA_FLAG_FREE;
 }
 
 
@@ -501,7 +556,19 @@ static inline void HEAP_InsertFreeBlock( HEAP *heap, ARENA_FREE *pArena, BOOL la
  */
 static inline void HEAP_DeleteFreeBlock( HEAP *heap, ARENA_FREE *pArena )
 {
-    list_remove( &pArena->entry );
+    if ((pArena->size & ARENA_FLAG_FREE) == ARENA_FLAG_FREE_LIST)
+    {
+        if (pArena->entry.list.prev == pArena->entry.list.next)
+        {
+            SIZE_T index = HEAP_SIZE_TO_FREELIST_INDEX( (pArena->size & ARENA_SIZE_MASK) + sizeof(*pArena) );
+            heap->freeMask[ HEAP_FREEMASK_INDEX( index ) ] &= ~HEAP_FREEMASK_BIT( index );
+        }
+        list_remove( &pArena->entry.list );
+    }
+    else
+    {
+        wine_rb_remove( &heap->freeTree, &pArena->entry.tree );
+    }
 }
 
 
@@ -881,6 +948,15 @@ static BOOL validate_large_arena( HEAP *heap, const ARENA_LARGE *arena, BOOL qui
 }
 
 
+static inline int arena_free_compare( const void *key, const struct wine_rb_entry *entry )
+{
+    DWORD arena_size = get_arena_size( entry );
+    if (*(DWORD *)key > arena_size) return 1;
+    else if (*(DWORD *)key < arena_size) return -1;
+    else return entry->left ? 1 : -1;
+}
+
+
 /***********************************************************************
  *           HEAP_CreateSubHeap
  */
@@ -888,7 +964,6 @@ static SUBHEAP *HEAP_CreateSubHeap( HEAP *heap, LPVOID address, DWORD flags,
                                     SIZE_T commitSize, SIZE_T totalSize )
 {
     SUBHEAP *subheap;
-    FREE_LIST_ENTRY *pEntry;
     unsigned int i;
 
     if (!address)
@@ -949,17 +1024,21 @@ static SUBHEAP *HEAP_CreateSubHeap( HEAP *heap, LPVOID address, DWORD flags,
         subheap->headerSize = ROUND_SIZE( sizeof(HEAP) );
         list_add_head( &heap->subheap_list, &subheap->entry );
 
-        /* Build the free lists */
+        /* Initialize the free tree */
 
-        heap->freeList = (FREE_LIST_ENTRY *)((char *)heap + subheap->headerSize);
-        subheap->headerSize += HEAP_NB_FREE_LISTS * sizeof(FREE_LIST_ENTRY);
-        list_init( &heap->freeList[0].arena.entry );
-        for (i = 0, pEntry = heap->freeList; i < HEAP_NB_FREE_LISTS; i++, pEntry++)
-        {
-            pEntry->arena.size = 0 | ARENA_FLAG_FREE;
-            pEntry->arena.magic = ARENA_FREE_MAGIC;
-            if (i) list_add_after( &pEntry[-1].arena.entry, &pEntry->arena.entry );
-        }
+        heap->freeList = (struct list *)((char *)heap + subheap->headerSize);
+        subheap->headerSize += HEAP_NB_FREE_LISTS * sizeof(struct list);
+        for (i = 0; i < HEAP_NB_FREE_LISTS; i++)
+            list_init( &heap->freeList[i] );
+
+        /* Initialize the free tree */
+
+        wine_rb_init( &heap->freeTree, arena_free_compare );
+
+        /* Initialize the free mask */
+
+        for (i = 0; i < sizeof(heap->freeMask) / sizeof(heap->freeMask[0]); i++)
+            heap->freeMask[i] = 0;
 
         /* Initialize critical section */
 
@@ -1002,6 +1081,34 @@ static SUBHEAP *HEAP_CreateSubHeap( HEAP *heap, LPVOID address, DWORD flags,
 }
 
 
+/* helper function for HEAP_FindFreeBlock */
+static struct wine_rb_entry *find_free_block( struct wine_rb_entry *entry, DWORD arena_size )
+{
+    for (;;)
+    {
+        if (!entry) return NULL;
+        if (get_arena_size( entry ) >= arena_size) break;
+        entry = entry->right;
+    }
+
+    for (;;)
+    {
+        if (!entry->left) return entry;
+        if (get_arena_size( entry->left ) < arena_size) break;
+        entry = entry->left;
+    }
+
+    if (entry->left->right)
+    {
+        struct wine_rb_entry *ret;
+        if ((ret = find_free_block( entry->left->right, arena_size )))
+            return ret;
+    }
+
+    return entry;
+}
+
+
 /***********************************************************************
  *           HEAP_FindFreeBlock
  *
@@ -1011,26 +1118,39 @@ static SUBHEAP *HEAP_CreateSubHeap( HEAP *heap, LPVOID address, DWORD flags,
 static ARENA_FREE *HEAP_FindFreeBlock( HEAP *heap, SIZE_T size,
                                        SUBHEAP **ppSubHeap )
 {
+    struct wine_rb_entry *ptr;
+    unsigned long mask;
+    ARENA_FREE *arena;
     SUBHEAP *subheap;
-    struct list *ptr;
     SIZE_T total_size;
-    FREE_LIST_ENTRY *pEntry = heap->freeList + get_freelist_index( size + sizeof(ARENA_INUSE) );
+    SIZE_T index = HEAP_SIZE_TO_FREELIST_INDEX( size + sizeof(ARENA_INUSE) );
 
-    /* Find a suitable free list, and in it find a block large enough */
+    /* Find a suitable block from the free list */
 
-    ptr = &pEntry->arena.entry;
-    while ((ptr = list_next( &heap->freeList[0].arena.entry, ptr )))
+    while (index < HEAP_NB_FREE_LISTS)
     {
-        ARENA_FREE *pArena = LIST_ENTRY( ptr, ARENA_FREE, entry );
-        SIZE_T arena_size = (pArena->size & ARENA_SIZE_MASK) +
-                            sizeof(ARENA_FREE) - sizeof(ARENA_INUSE);
-        if (arena_size >= size)
+        mask = heap->freeMask[ HEAP_FREEMASK_INDEX( index ) ] & ~(HEAP_FREEMASK_BIT( index ) - 1);
+        if (mask)
         {
-            subheap = HEAP_FindSubHeap( heap, pArena );
-            if (!HEAP_Commit( subheap, (ARENA_INUSE *)pArena, size )) return NULL;
+            index = (index & ~(HEAP_FREEMASK_BLOCK - 1)) | ctzl( mask );
+            arena = LIST_ENTRY( heap->freeList[index].next, ARENA_FREE, entry.list );
+            subheap = HEAP_FindSubHeap( heap, arena );
+            if (!HEAP_Commit( subheap, (ARENA_INUSE *)arena, size )) return NULL;
             *ppSubHeap = subheap;
-            return pArena;
+            return arena;
         }
+        index = (index + HEAP_FREEMASK_BLOCK) & ~(HEAP_FREEMASK_BLOCK - 1);
+    }
+
+    /* Find a suitable block from the free tree */
+
+    if ((ptr = find_free_block( heap->freeTree.root, size + sizeof(ARENA_INUSE) - sizeof(ARENA_FREE) )))
+    {
+        arena = WINE_RB_ENTRY_VALUE( ptr, ARENA_FREE, entry.tree );
+        subheap = HEAP_FindSubHeap( heap, arena );
+        if (!HEAP_Commit( subheap, (ARENA_INUSE *)arena, size )) return NULL;
+        *ppSubHeap = subheap;
+        return arena;
     }
 
     /* If no block was found, attempt to grow the heap */
@@ -1076,13 +1196,10 @@ static ARENA_FREE *HEAP_FindFreeBlock( HEAP *heap, SIZE_T size,
  */
 static BOOL HEAP_IsValidArenaPtr( const HEAP *heap, const ARENA_FREE *ptr )
 {
-    unsigned int i;
     const SUBHEAP *subheap = HEAP_FindSubHeap( heap, ptr );
     if (!subheap) return FALSE;
     if ((const char *)ptr >= (const char *)subheap->base + subheap->headerSize) return TRUE;
     if (subheap != &heap->subheap) return FALSE;
-    for (i = 0; i < HEAP_NB_FREE_LISTS; i++)
-        if (ptr == &heap->freeList[i].arena) return TRUE;
     return FALSE;
 }
 
@@ -1094,7 +1211,7 @@ static BOOL HEAP_ValidateFreeArena( SUBHEAP *subheap, ARENA_FREE *pArena )
 {
     DWORD flags = subheap->heap->flags;
     SIZE_T size;
-    ARENA_FREE *prev, *next;
+    ARENA_FREE *prev = NULL, *next = NULL;
     char *heapEnd = (char *)subheap->base + subheap->size;
 
     /* Check for unaligned pointers */
@@ -1111,7 +1228,8 @@ static BOOL HEAP_ValidateFreeArena( SUBHEAP *subheap, ARENA_FREE *pArena )
         return FALSE;
     }
     /* Check size flags */
-    if (!(pArena->size & ARENA_FLAG_FREE) ||
+    if (((pArena->size & ARENA_FLAG_FREE) != ARENA_FLAG_FREE_LIST &&
+         (pArena->size & ARENA_FLAG_FREE) != ARENA_FLAG_FREE_TREE) ||
         (pArena->size & ARENA_FLAG_PREV_FREE))
     {
         ERR("Heap %p: bad flags %08x for free arena %p\n",
@@ -1125,34 +1243,45 @@ static BOOL HEAP_ValidateFreeArena( SUBHEAP *subheap, ARENA_FREE *pArena )
         ERR("Heap %p: bad size %08lx for free arena %p\n", subheap->heap, size, pArena );
         return FALSE;
     }
+    if ((pArena->size & ARENA_FLAG_FREE) == ARENA_FLAG_FREE_LIST)
+    {
+        struct list *list = &subheap->heap->freeList[ HEAP_SIZE_TO_FREELIST_INDEX( size + sizeof(*pArena) ) ];
+        if (pArena->entry.list.prev != list)
+            prev = LIST_ENTRY( pArena->entry.list.prev, ARENA_FREE, entry.list );
+        if (pArena->entry.list.next != list)
+            next = LIST_ENTRY( pArena->entry.list.next, ARENA_FREE, entry.list );
+    }
+    else
+    {
+        if (pArena->entry.tree.left)
+            prev = WINE_RB_ENTRY_VALUE( pArena->entry.tree.left, ARENA_FREE, entry.tree );
+        if (pArena->entry.tree.right)
+            next = WINE_RB_ENTRY_VALUE( pArena->entry.tree.right, ARENA_FREE, entry.tree );
+    }
     /* Check that next pointer is valid */
-    next = LIST_ENTRY( pArena->entry.next, ARENA_FREE, entry );
-    if (!HEAP_IsValidArenaPtr( subheap->heap, next ))
+    if (next && !HEAP_IsValidArenaPtr( subheap->heap, next ))
     {
         ERR("Heap %p: bad next ptr %p for arena %p\n",
             subheap->heap, next, pArena );
         return FALSE;
     }
     /* Check that next arena is free */
-    if (!(next->size & ARENA_FLAG_FREE) || (next->magic != ARENA_FREE_MAGIC))
+    if (next && (!(next->size & ARENA_FLAG_FREE) || (next->magic != ARENA_FREE_MAGIC)))
     {
         ERR("Heap %p: next arena %p invalid for %p\n",
             subheap->heap, next, pArena );
         return FALSE;
     }
     /* Check that prev pointer is valid */
-    prev = LIST_ENTRY( pArena->entry.prev, ARENA_FREE, entry );
-    if (!HEAP_IsValidArenaPtr( subheap->heap, prev ))
+    if (prev && !HEAP_IsValidArenaPtr( subheap->heap, prev ))
     {
         ERR("Heap %p: bad prev ptr %p for arena %p\n",
             subheap->heap, prev, pArena );
         return FALSE;
     }
     /* Check that prev arena is free */
-    if (!(prev->size & ARENA_FLAG_FREE) || (prev->magic != ARENA_FREE_MAGIC))
+    if (prev && (!(prev->size & ARENA_FLAG_FREE) || (prev->magic != ARENA_FREE_MAGIC)))
     {
-	/* this often means that the prev arena got overwritten
-	 * by a memory write before that prev arena */
         ERR("Heap %p: prev arena %p invalid for %p\n",
             subheap->heap, prev, pArena );
         return FALSE;
From 35bd7c6f0c407bad59cdc406d5085c08b05ccb88 Mon Sep 17 00:00:00 2001
From: Sebastian Lackner <sebastian@fds-team.de>
Date: Sat, 5 Aug 2017 03:37:47 +0200
Subject: [PATCH] include: Move interlocked_inc/dec to port.h.

---
 dlls/ntdll/critsection.c | 10 ----------
 dlls/ntdll/threadpool.c  | 10 ----------
 include/wine/port.h      | 14 +++++++++++++-
 3 files changed, 13 insertions(+), 21 deletions(-)

diff --git a/dlls/ntdll/critsection.c b/dlls/ntdll/critsection.c
index fb616adfd87..d9add3f0607 100644
--- a/dlls/ntdll/critsection.c
+++ b/dlls/ntdll/critsection.c
@@ -40,16 +40,6 @@
 WINE_DEFAULT_DEBUG_CHANNEL(ntdll);
 WINE_DECLARE_DEBUG_CHANNEL(relay);
 
-static inline LONG interlocked_inc( PLONG dest )
-{
-    return interlocked_xchg_add( dest, 1 ) + 1;
-}
-
-static inline LONG interlocked_dec( PLONG dest )
-{
-    return interlocked_xchg_add( dest, -1 ) - 1;
-}
-
 static inline void small_pause(void)
 {
 #ifdef __i386__
diff --git a/dlls/ntdll/threadpool.c b/dlls/ntdll/threadpool.c
index bf7449cdfa6..b8dc8273702 100644
--- a/dlls/ntdll/threadpool.c
+++ b/dlls/ntdll/threadpool.c
@@ -331,16 +331,6 @@ static void tp_object_prepare_shutdown( struct threadpool_object *object );
 static BOOL tp_object_release( struct threadpool_object *object );
 static struct threadpool *default_threadpool = NULL;
 
-static inline LONG interlocked_inc( PLONG dest )
-{
-    return interlocked_xchg_add( dest, 1 ) + 1;
-}
-
-static inline LONG interlocked_dec( PLONG dest )
-{
-    return interlocked_xchg_add( dest, -1 ) - 1;
-}
-
 static void CALLBACK process_rtl_work_item( TP_CALLBACK_INSTANCE *instance, void *userdata )
 {
     struct rtl_work_item *item = userdata;
diff --git a/include/wine/port.h b/include/wine/port.h
index fb7251edd6c..a3b6c6fb99a 100644
--- a/include/wine/port.h
+++ b/include/wine/port.h
@@ -518,6 +518,16 @@ static inline __int64 interlocked_cmpxchg64( __int64 *dest, __int64 xchg, __int6
 extern __int64 interlocked_cmpxchg64( __int64 *dest, __int64 xchg, __int64 compare );
 #endif
 
+static inline int interlocked_inc( int *dest )
+{
+    return interlocked_xchg_add( dest, 1 ) + 1;
+}
+
+static inline int interlocked_dec( int *dest )
+{
+    return interlocked_xchg_add( dest, -1 ) - 1;
+}
+
 #else /* NO_LIBWINE_PORT */
 
 #define __WINE_NOT_PORTABLE(func) func##_is_not_portable func##_is_not_portable
@@ -528,9 +538,11 @@ extern __int64 interlocked_cmpxchg64( __int64 *dest, __int64 xchg, __int64 compa
 #define getopt_long_only        __WINE_NOT_PORTABLE(getopt_long_only)
 #define interlocked_cmpxchg     __WINE_NOT_PORTABLE(interlocked_cmpxchg)
 #define interlocked_cmpxchg_ptr __WINE_NOT_PORTABLE(interlocked_cmpxchg_ptr)
+#define interlocked_dec         __WINE_NOT_PORTABLE(interlocked_dec)
+#define interlocked_inc         __WINE_NOT_PORTABLE(interlocked_inc)
 #define interlocked_xchg        __WINE_NOT_PORTABLE(interlocked_xchg)
-#define interlocked_xchg_ptr    __WINE_NOT_PORTABLE(interlocked_xchg_ptr)
 #define interlocked_xchg_add    __WINE_NOT_PORTABLE(interlocked_xchg_add)
+#define interlocked_xchg_ptr    __WINE_NOT_PORTABLE(interlocked_xchg_ptr)
 #define lstat                   __WINE_NOT_PORTABLE(lstat)
 #define memcpy_unaligned        __WINE_NOT_PORTABLE(memcpy_unaligned)
 #undef memmove
From 3a5edb56e6a24cee23ad0e893a8f2efc26707e51 Mon Sep 17 00:00:00 2001
From: Sebastian Lackner <sebastian@fds-team.de>
Date: Sat, 5 Aug 2017 03:38:38 +0200
Subject: [PATCH] ntdll: Add inline versions of RtlEnterCriticalSection /
 RtlLeaveCriticalSections.

---
 dlls/ntdll/ntdll_misc.h | 38 ++++++++++++++++++++++++++++++++++++++
 1 file changed, 38 insertions(+)

diff --git a/dlls/ntdll/ntdll_misc.h b/dlls/ntdll/ntdll_misc.h
index b0c1c80e47f..cf0d50de720 100644
--- a/dlls/ntdll/ntdll_misc.h
+++ b/dlls/ntdll/ntdll_misc.h
@@ -27,6 +27,7 @@
 #include "windef.h"
 #include "winnt.h"
 #include "winternl.h"
+#include "wine/debug.h"
 #include "wine/server.h"
 
 #define MAX_NT_PATH_LENGTH 277
@@ -205,6 +206,43 @@ extern int ntdll_wcstoumbs(DWORD flags, const WCHAR* src, int srclen, char* dst,
 extern int CDECL NTDLL__vsnprintf( char *str, SIZE_T len, const char *format, __ms_va_list args ) DECLSPEC_HIDDEN;
 extern int CDECL NTDLL__vsnwprintf( WCHAR *str, SIZE_T len, const WCHAR *format, __ms_va_list args ) DECLSPEC_HIDDEN;
 
+#ifdef __WINE_WINE_PORT_H
+
+/* inline version of RtlEnterCriticalSection */
+static inline void enter_critical_section( RTL_CRITICAL_SECTION *crit )
+{
+    if (interlocked_inc( &crit->LockCount ))
+    {
+        if (crit->OwningThread == ULongToHandle(GetCurrentThreadId()))
+        {
+            crit->RecursionCount++;
+            return;
+        }
+        RtlpWaitForCriticalSection( crit );
+    }
+    crit->OwningThread   = ULongToHandle(GetCurrentThreadId());
+    crit->RecursionCount = 1;
+}
+
+/* inline version of RtlLeaveCriticalSection */
+static inline void leave_critical_section( RTL_CRITICAL_SECTION *crit )
+{
+    WINE_DECLARE_DEBUG_CHANNEL(ntdll);
+    if (--crit->RecursionCount)
+    {
+        if (crit->RecursionCount > 0) interlocked_dec( &crit->LockCount );
+        else ERR_(ntdll)( "section %p is not acquired\n", crit );
+    }
+    else
+    {
+        crit->OwningThread = 0;
+        if (interlocked_dec( &crit->LockCount ) >= 0)
+            RtlpUnWaitCriticalSection( crit );
+    }
+}
+
+#endif  /* __WINE_WINE_PORT_H */
+
 /* load order */
 
 enum loadorder
From 7d8dfcf624553b8d5ac6e461c4590fcebf088e58 Mon Sep 17 00:00:00 2001
From: Sebastian Lackner <sebastian@fds-team.de>
Date: Sat, 5 Aug 2017 03:39:37 +0200
Subject: [PATCH] ntdll: Use fast CS functions for threadpool locking.

According to Steven Noonan, these patches make profiling easier.
---
 dlls/ntdll/threadpool.c | 96 ++++++++++++++++++++---------------------
 1 file changed, 48 insertions(+), 48 deletions(-)

diff --git a/dlls/ntdll/threadpool.c b/dlls/ntdll/threadpool.c
index b8dc8273702..6209cdaa8af 100644
--- a/dlls/ntdll/threadpool.c
+++ b/dlls/ntdll/threadpool.c
@@ -1205,7 +1205,7 @@ static void CALLBACK timerqueue_thread_proc( void *param )
 
     TRACE( "starting timer queue thread\n" );
 
-    RtlEnterCriticalSection( &timerqueue.cs );
+    enter_critical_section( &timerqueue.cs );
     for (;;)
     {
         NtQuerySystemTime( &now );
@@ -1279,7 +1279,7 @@ static void CALLBACK timerqueue_thread_proc( void *param )
     }
 
     timerqueue.thread_running = FALSE;
-    RtlLeaveCriticalSection( &timerqueue.cs );
+    leave_critical_section( &timerqueue.cs );
 
     TRACE( "terminating timer queue thread\n" );
     RtlExitUserThread( 0 );
@@ -1325,7 +1325,7 @@ static NTSTATUS tp_timerqueue_lock( struct threadpool_object *timer )
     timer->u.timer.period               = 0;
     timer->u.timer.window_length        = 0;
 
-    RtlEnterCriticalSection( &timerqueue.cs );
+    enter_critical_section( &timerqueue.cs );
 
     /* Make sure that the timerqueue thread is running. */
     if (!timerqueue.thread_running)
@@ -1346,7 +1346,7 @@ static NTSTATUS tp_timerqueue_lock( struct threadpool_object *timer )
         timerqueue.objcount++;
     }
 
-    RtlLeaveCriticalSection( &timerqueue.cs );
+    leave_critical_section( &timerqueue.cs );
     return status;
 }
 
@@ -1359,7 +1359,7 @@ static void tp_timerqueue_unlock( struct threadpool_object *timer )
 {
     assert( timer->type == TP_OBJECT_TYPE_TIMER );
 
-    RtlEnterCriticalSection( &timerqueue.cs );
+    enter_critical_section( &timerqueue.cs );
     if (timer->u.timer.timer_initialized)
     {
         /* If timer was pending, remove it. */
@@ -1378,7 +1378,7 @@ static void tp_timerqueue_unlock( struct threadpool_object *timer )
 
         timer->u.timer.timer_initialized = FALSE;
     }
-    RtlLeaveCriticalSection( &timerqueue.cs );
+    leave_critical_section( &timerqueue.cs );
 }
 
 /***********************************************************************
@@ -1396,7 +1396,7 @@ static void CALLBACK waitqueue_thread_proc( void *param )
 
     TRACE( "starting wait queue thread\n" );
 
-    RtlEnterCriticalSection( &waitqueue.cs );
+    enter_critical_section( &waitqueue.cs );
 
     for (;;)
     {
@@ -1433,10 +1433,10 @@ static void CALLBACK waitqueue_thread_proc( void *param )
             /* All wait objects have been destroyed, if no new wait objects are created
              * within some amount of time, then we can shutdown this thread. */
             assert( num_handles == 0 );
-            RtlLeaveCriticalSection( &waitqueue.cs );
+            leave_critical_section( &waitqueue.cs );
             timeout.QuadPart = (ULONGLONG)THREADPOOL_WORKER_TIMEOUT * -10000;
             status = NtWaitForMultipleObjects( 1, &bucket->update_event, TRUE, FALSE, &timeout );
-            RtlEnterCriticalSection( &waitqueue.cs );
+            enter_critical_section( &waitqueue.cs );
 
             if (status == STATUS_TIMEOUT && !bucket->objcount)
                 break;
@@ -1444,9 +1444,9 @@ static void CALLBACK waitqueue_thread_proc( void *param )
         else
         {
             handles[num_handles] = bucket->update_event;
-            RtlLeaveCriticalSection( &waitqueue.cs );
+            leave_critical_section( &waitqueue.cs );
             status = NtWaitForMultipleObjects( num_handles + 1, handles, TRUE, FALSE, &timeout );
-            RtlEnterCriticalSection( &waitqueue.cs );
+            enter_critical_section( &waitqueue.cs );
 
             if (status >= STATUS_WAIT_0 && status < STATUS_WAIT_0 + num_handles)
             {
@@ -1519,7 +1519,7 @@ static void CALLBACK waitqueue_thread_proc( void *param )
     if (!--waitqueue.num_buckets)
         assert( list_empty( &waitqueue.buckets ) );
 
-    RtlLeaveCriticalSection( &waitqueue.cs );
+    leave_critical_section( &waitqueue.cs );
 
     TRACE( "terminating wait queue thread\n" );
 
@@ -1548,7 +1548,7 @@ static NTSTATUS tp_waitqueue_lock( struct threadpool_object *wait )
     wait->u.wait.timeout        = 0;
     wait->u.wait.handle         = INVALID_HANDLE_VALUE;
 
-    RtlEnterCriticalSection( &waitqueue.cs );
+    enter_critical_section( &waitqueue.cs );
 
     /* Try to assign to existing bucket if possible. */
     LIST_FOR_EACH_ENTRY( bucket, &waitqueue.buckets, struct waitqueue_bucket, bucket_entry )
@@ -1604,7 +1604,7 @@ static NTSTATUS tp_waitqueue_lock( struct threadpool_object *wait )
     }
 
 out:
-    RtlLeaveCriticalSection( &waitqueue.cs );
+    leave_critical_section( &waitqueue.cs );
     return status;
 }
 
@@ -1615,7 +1615,7 @@ static void tp_waitqueue_unlock( struct threadpool_object *wait )
 {
     assert( wait->type == TP_OBJECT_TYPE_WAIT );
 
-    RtlEnterCriticalSection( &waitqueue.cs );
+    enter_critical_section( &waitqueue.cs );
     if (wait->u.wait.bucket)
     {
         struct waitqueue_bucket *bucket = wait->u.wait.bucket;
@@ -1627,7 +1627,7 @@ static void tp_waitqueue_unlock( struct threadpool_object *wait )
 
         NtSetEvent( bucket->update_event, NULL );
     }
-    RtlLeaveCriticalSection( &waitqueue.cs );
+    leave_critical_section( &waitqueue.cs );
 }
 
 /***********************************************************************
@@ -1735,7 +1735,7 @@ static NTSTATUS tp_threadpool_lock( struct threadpool **out, TP_CALLBACK_ENVIRON
         pool = default_threadpool;
     }
 
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
 
     /* Make sure that the threadpool has at least one thread. */
     if (!pool->num_workers)
@@ -1749,7 +1749,7 @@ static NTSTATUS tp_threadpool_lock( struct threadpool **out, TP_CALLBACK_ENVIRON
         pool->objcount++;
     }
 
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
 
     if (status != STATUS_SUCCESS)
         return status;
@@ -1765,9 +1765,9 @@ static NTSTATUS tp_threadpool_lock( struct threadpool **out, TP_CALLBACK_ENVIRON
  */
 static void tp_threadpool_unlock( struct threadpool *pool )
 {
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
     pool->objcount--;
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
     tp_threadpool_release( pool );
 }
 
@@ -1896,10 +1896,10 @@ static void tp_object_initialize( struct threadpool_object *object, struct threa
         struct threadpool_group *group = object->group;
         interlocked_inc( &group->refcount );
 
-        RtlEnterCriticalSection( &group->cs );
+        enter_critical_section( &group->cs );
         list_add_tail( &group->members, &object->group_entry );
         object->is_group_member = TRUE;
-        RtlLeaveCriticalSection( &group->cs );
+        leave_critical_section( &group->cs );
     }
 
     if (is_simple_callback)
@@ -1920,7 +1920,7 @@ static void tp_object_submit( struct threadpool_object *object, BOOL signaled )
     assert( !object->shutdown );
     assert( !pool->shutdown );
 
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
 
     /* Start new worker threads if required. */
     if (pool->num_busy_workers >= pool->num_workers &&
@@ -1943,7 +1943,7 @@ static void tp_object_submit( struct threadpool_object *object, BOOL signaled )
         RtlWakeConditionVariable( &pool->update_event );
     }
 
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
 }
 
 /***********************************************************************
@@ -1956,7 +1956,7 @@ static void tp_object_cancel( struct threadpool_object *object )
     struct threadpool *pool = object->pool;
     LONG pending_callbacks = 0;
 
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
     if (object->num_pending_callbacks)
     {
         pending_callbacks = object->num_pending_callbacks;
@@ -1966,7 +1966,7 @@ static void tp_object_cancel( struct threadpool_object *object )
         if (object->type == TP_OBJECT_TYPE_WAIT)
             object->u.wait.signaled = 0;
     }
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
 
     while (pending_callbacks--)
         tp_object_release( object );
@@ -1982,7 +1982,7 @@ static void tp_object_wait( struct threadpool_object *object, BOOL group_wait )
 {
     struct threadpool *pool = object->pool;
 
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
     if (group_wait)
     {
         while (object->num_pending_callbacks || object->num_running_callbacks)
@@ -1993,7 +1993,7 @@ static void tp_object_wait( struct threadpool_object *object, BOOL group_wait )
         while (object->num_pending_callbacks || object->num_associated_callbacks)
             RtlSleepConditionVariableCS( &object->finished_event, &pool->cs, NULL );
     }
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
 }
 
 /***********************************************************************
@@ -2031,13 +2031,13 @@ static BOOL tp_object_release( struct threadpool_object *object )
     {
         struct threadpool_group *group = object->group;
 
-        RtlEnterCriticalSection( &group->cs );
+        enter_critical_section( &group->cs );
         if (object->is_group_member)
         {
             list_remove( &object->group_entry );
             object->is_group_member = FALSE;
         }
-        RtlLeaveCriticalSection( &group->cs );
+        leave_critical_section( &group->cs );
 
         tp_group_release( group );
     }
@@ -2066,7 +2066,7 @@ static void CALLBACK threadpool_worker_proc( void *param )
 
     TRACE( "starting worker thread for pool %p\n", pool );
 
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
     pool->num_busy_workers--;
     for (;;)
     {
@@ -2092,7 +2092,7 @@ static void CALLBACK threadpool_worker_proc( void *param )
             object->num_associated_callbacks++;
             object->num_running_callbacks++;
             pool->num_busy_workers++;
-            RtlLeaveCriticalSection( &pool->cs );
+            leave_critical_section( &pool->cs );
 
             /* Initialize threadpool instance struct. */
             callback_instance = (TP_CALLBACK_INSTANCE *)&instance;
@@ -2185,7 +2185,7 @@ static void CALLBACK threadpool_worker_proc( void *param )
             }
 
         skip_cleanup:
-            RtlEnterCriticalSection( &pool->cs );
+            enter_critical_section( &pool->cs );
             pool->num_busy_workers--;
 
             /* Simple callbacks are automatically shutdown after execution. */
@@ -2227,7 +2227,7 @@ static void CALLBACK threadpool_worker_proc( void *param )
         }
     }
     pool->num_workers--;
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
 
     TRACE( "terminating worker thread for pool %p\n", pool );
     tp_threadpool_release( pool );
@@ -2403,7 +2403,7 @@ NTSTATUS WINAPI TpCallbackMayRunLong( TP_CALLBACK_INSTANCE *instance )
         return STATUS_SUCCESS;
 
     pool = object->pool;
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
 
     /* Start new worker threads if required. */
     if (pool->num_busy_workers >= pool->num_workers)
@@ -2418,7 +2418,7 @@ NTSTATUS WINAPI TpCallbackMayRunLong( TP_CALLBACK_INSTANCE *instance )
         }
     }
 
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
     this->may_run_long = TRUE;
     return status;
 }
@@ -2499,13 +2499,13 @@ VOID WINAPI TpDisassociateCallback( TP_CALLBACK_INSTANCE *instance )
         return;
 
     pool = object->pool;
-    RtlEnterCriticalSection( &pool->cs );
+    enter_critical_section( &pool->cs );
 
     object->num_associated_callbacks--;
     if (!object->num_pending_callbacks && !object->num_associated_callbacks)
         RtlWakeAllConditionVariable( &object->finished_event );
 
-    RtlLeaveCriticalSection( &pool->cs );
+    leave_critical_section( &pool->cs );
     this->associated = FALSE;
 }
 
@@ -2557,7 +2557,7 @@ VOID WINAPI TpReleaseCleanupGroupMembers( TP_CLEANUP_GROUP *group, BOOL cancel_p
 
     TRACE( "%p %u %p\n", group, cancel_pending, userdata );
 
-    RtlEnterCriticalSection( &this->cs );
+    enter_critical_section( &this->cs );
 
     /* Unset group, increase references, and mark objects for shutdown */
     LIST_FOR_EACH_ENTRY_SAFE( object, next, &this->members, struct threadpool_object, group_entry )
@@ -2583,7 +2583,7 @@ VOID WINAPI TpReleaseCleanupGroupMembers( TP_CLEANUP_GROUP *group, BOOL cancel_p
     list_init( &members );
     list_move_tail( &members, &this->members );
 
-    RtlLeaveCriticalSection( &this->cs );
+    leave_critical_section( &this->cs );
 
     /* Cancel pending callbacks if requested */
     if (cancel_pending)
@@ -2683,10 +2683,10 @@ VOID WINAPI TpSetPoolMaxThreads( TP_POOL *pool, DWORD maximum )
 
     TRACE( "%p %u\n", pool, maximum );
 
-    RtlEnterCriticalSection( &this->cs );
+    enter_critical_section( &this->cs );
     this->max_workers = max( maximum, 1 );
     this->min_workers = min( this->min_workers, this->max_workers );
-    RtlLeaveCriticalSection( &this->cs );
+    leave_critical_section( &this->cs );
 }
 
 /***********************************************************************
@@ -2699,7 +2699,7 @@ BOOL WINAPI TpSetPoolMinThreads( TP_POOL *pool, DWORD minimum )
 
     TRACE( "%p %u\n", pool, minimum );
 
-    RtlEnterCriticalSection( &this->cs );
+    enter_critical_section( &this->cs );
 
     while (this->num_workers < minimum)
     {
@@ -2714,7 +2714,7 @@ BOOL WINAPI TpSetPoolMinThreads( TP_POOL *pool, DWORD minimum )
         this->max_workers = max( this->min_workers, this->max_workers );
     }
 
-    RtlLeaveCriticalSection( &this->cs );
+    leave_critical_section( &this->cs );
     return !status;
 }
 
@@ -2730,7 +2730,7 @@ VOID WINAPI TpSetTimer( TP_TIMER *timer, LARGE_INTEGER *timeout, LONG period, LO
 
     TRACE( "%p %p %u %u\n", timer, timeout, period, window_length );
 
-    RtlEnterCriticalSection( &timerqueue.cs );
+    enter_critical_section( &timerqueue.cs );
 
     assert( this->u.timer.timer_initialized );
     this->u.timer.timer_set = timeout != NULL;
@@ -2790,7 +2790,7 @@ VOID WINAPI TpSetTimer( TP_TIMER *timer, LARGE_INTEGER *timeout, LONG period, LO
         this->u.timer.timer_pending = TRUE;
     }
 
-    RtlLeaveCriticalSection( &timerqueue.cs );
+    leave_critical_section( &timerqueue.cs );
 
     if (submit_timer)
        tp_object_submit( this, FALSE );
@@ -2807,7 +2807,7 @@ VOID WINAPI TpSetWait( TP_WAIT *wait, HANDLE handle, LARGE_INTEGER *timeout )
 
     TRACE( "%p %p %p\n", wait, handle, timeout );
 
-    RtlEnterCriticalSection( &waitqueue.cs );
+    enter_critical_section( &waitqueue.cs );
 
     assert( this->u.wait.bucket );
     this->u.wait.handle = handle;
@@ -2851,7 +2851,7 @@ VOID WINAPI TpSetWait( TP_WAIT *wait, HANDLE handle, LARGE_INTEGER *timeout )
         NtSetEvent( bucket->update_event, NULL );
     }
 
-    RtlLeaveCriticalSection( &waitqueue.cs );
+    leave_critical_section( &waitqueue.cs );
 
     if (submit_wait)
         tp_object_submit( this, FALSE );
